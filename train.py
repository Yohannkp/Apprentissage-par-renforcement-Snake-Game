import os
import numpy as np
import matplotlib.pyplot as plt
from env.snake_game import SnakeGame
from agent.dqn_agent import DQNAgent
from tqdm import tqdm
import torch

def train_agent(n_episodes=2000, save_interval=100, model_path="models/snake_dqn.pth"):
    """
    Entra√Æne l'agent DQN sur le jeu Snake.
    
    Args:
        n_episodes: Nombre d'√©pisodes d'entra√Ænement
        save_interval: Intervalle de sauvegarde du mod√®le
        model_path: Chemin de sauvegarde du mod√®le
    """
    
    # Cr√©er le dossier models s'il n'existe pas
    os.makedirs("models", exist_ok=True)
    
    # Initialiser l'environnement et l'agent
    env = SnakeGame(width=640, height=480, block_size=20, speed=50)
    agent = DQNAgent(
        state_size=11,
        action_size=3,
        lr=0.001,
        gamma=0.9,
        epsilon=1.0,
        epsilon_min=0.01,
        epsilon_decay=0.995,
        memory_size=10000,
        batch_size=32,
        target_update=1000
    )
    
    # M√©triques d'entra√Ænement
    scores = []
    mean_scores = []
    total_score = 0
    best_score = 0
    
    print("üöÄ D√âBUT DE L'ENTRA√éNEMENT ACC√âL√âR√â GPU")
    print("="*50)
    
    # Afficher les informations GPU avant l'entra√Ænement
    if torch.cuda.is_available():
        gpu_name = torch.cuda.get_device_name(0)
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
        print(f"üéÆ GPU utilis√© : {gpu_name}")
        print(f"üíæ M√©moire GPU : {gpu_memory:.1f} GB")
        
        # V√©rifier si c'est la RTX 4060
        if "4060" in gpu_name:
            print("‚úÖ RTX 4060 d√©tect√©e - Performance optimale !")
        
        print(f"‚ö° Entra√Ænement acc√©l√©r√© GPU activ√©")
    else:
        print("‚ö†Ô∏è  GPU non disponible - Utilisation CPU (plus lent)")
    
    print(f"üìä Information du mod√®le: {agent.get_model_info()}")
    print("="*50)
    
    for episode in tqdm(range(n_episodes), desc="Entra√Ænement"):
        # R√©initialiser l'environnement
        state, _ = env.reset()
        total_reward = 0
        done = False
        
        while not done:
            # Choisir une action
            action = agent.get_action(state, training=True)
            
            # Ex√©cuter l'action
            next_state, reward, done, _, info = env.step(action)
            
            # Stocker l'exp√©rience
            agent.remember(state, action, reward, next_state, done)
            
            # Passer √† l'√©tat suivant
            state = next_state
            total_reward += reward
            
            # Entra√Æner l'agent
            agent.train()
        
        # Statistiques
        score = info['score']
        scores.append(score)
        total_score += score
        mean_score = total_score / (episode + 1)
        mean_scores.append(mean_score)
        
        # Mise √† jour du meilleur score
        if score > best_score:
            best_score = score
            # Sauvegarder le meilleur mod√®le
            agent.save_model(model_path.replace('.pth', '_best.pth'))
        
        # Affichage p√©riodique
        if episode % 100 == 0:
            print(f"\n√âpisode {episode}")
            print(f"Score: {score}, Score moyen: {mean_score:.2f}")
            print(f"Meilleur score: {best_score}")
            print(f"Epsilon: {agent.epsilon:.3f}")
            print(f"Taille buffer: {len(agent.memory)}")
        
        # Sauvegarde p√©riodique
        if episode % save_interval == 0 and episode > 0:
            agent.save_model(model_path.replace('.pth', f'_episode_{episode}.pth'))
    
    # Sauvegarde finale
    agent.save_model(model_path)
    
    # Fermer l'environnement
    env.close()
    
    # Afficher les r√©sultats finaux
    print(f"\nEntra√Ænement termin√©!")
    print(f"Score final moyen: {mean_score:.2f}")
    print(f"Meilleur score atteint: {best_score}")
    
    # Sauvegarder les m√©triques
    np.save("models/training_scores.npy", scores)
    np.save("models/training_mean_scores.npy", mean_scores)
    
    return scores, mean_scores, agent

def plot_training_results(scores, mean_scores, save_path="models/training_plot.png"):
    """
    Affiche et sauvegarde les r√©sultats d'entra√Ænement.
    
    Args:
        scores: Liste des scores par √©pisode
        mean_scores: Liste des scores moyens
        save_path: Chemin de sauvegarde du graphique
    """
    plt.figure(figsize=(12, 8))
    
    # Subplot 1: Scores par √©pisode
    plt.subplot(2, 1, 1)
    plt.plot(scores, alpha=0.6, color='blue', label='Score par √©pisode')
    plt.plot(mean_scores, color='red', linewidth=2, label='Score moyen')
    plt.xlabel('√âpisode')
    plt.ylabel('Score')
    plt.title('√âvolution du score pendant l\'entra√Ænement')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # Subplot 2: Moyenne mobile des 100 derniers √©pisodes
    plt.subplot(2, 1, 2)
    if len(scores) >= 100:
        moving_avg = []
        for i in range(len(scores)):
            if i >= 99:
                moving_avg.append(np.mean(scores[i-99:i+1]))
            else:
                moving_avg.append(np.mean(scores[:i+1]))
        plt.plot(moving_avg, color='green', linewidth=2, label='Moyenne mobile (100 √©pisodes)')
    else:
        plt.plot(mean_scores, color='green', linewidth=2, label='Score moyen')
    
    plt.xlabel('√âpisode')
    plt.ylabel('Score moyen')
    plt.title('Moyenne mobile des scores')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.show()
    
    print(f"Graphique sauvegard√©: {save_path}")

def resume_training(model_path, additional_episodes=1000):
    """
    Reprend l'entra√Ænement √† partir d'un mod√®le sauvegard√©.
    
    Args:
        model_path: Chemin vers le mod√®le √† charger
        additional_episodes: Nombre d'√©pisodes suppl√©mentaires
    """
    # Charger les donn√©es pr√©c√©dentes si elles existent
    try:
        previous_scores = np.load("models/training_scores.npy").tolist()
        previous_mean_scores = np.load("models/training_mean_scores.npy").tolist()
        print(f"Chargement des m√©triques pr√©c√©dentes: {len(previous_scores)} √©pisodes")
    except FileNotFoundError:
        previous_scores = []
        previous_mean_scores = []
        print("Aucune m√©trique pr√©c√©dente trouv√©e, d√©marrage √† z√©ro")
    
    # Initialiser l'environnement et l'agent
    env = SnakeGame()
    agent = DQNAgent()
    
    # Charger le mod√®le
    agent.load_model(model_path)
    
    # Continuer l'entra√Ænement
    print(f"Reprise de l'entra√Ænement pour {additional_episodes} √©pisodes suppl√©mentaires...")
    
    new_scores, new_mean_scores, _ = train_agent(
        n_episodes=additional_episodes,
        save_interval=100,
        model_path=model_path
    )
    
    # Combiner avec les r√©sultats pr√©c√©dents
    all_scores = previous_scores + new_scores
    all_mean_scores = previous_mean_scores + new_mean_scores
    
    # Afficher les r√©sultats
    plot_training_results(all_scores, all_mean_scores)
    
    return all_scores, all_mean_scores

if __name__ == "__main__":
    # Configuration d'entra√Ænement
    EPISODES = 2000
    MODEL_PATH = "models/snake_dqn.pth"
    
    print("=== ENTRA√éNEMENT DE L'AGENT SNAKE DQN ===")
    print(f"Nombre d'√©pisodes: {EPISODES}")
    print(f"Chemin du mod√®le: {MODEL_PATH}")
    
    # Entra√Æner l'agent
    scores, mean_scores, trained_agent = train_agent(
        n_episodes=EPISODES,
        save_interval=200,
        model_path=MODEL_PATH
    )
    
    # Afficher les r√©sultats
    plot_training_results(scores, mean_scores)
    
    print("\nEntra√Ænement termin√©! Utilisez test.py pour voir l'agent jouer.")
