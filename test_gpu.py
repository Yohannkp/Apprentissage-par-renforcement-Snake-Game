#!/usr/bin/env python3
"""
Script de test pour v√©rifier l'utilisation optimale du GPU RTX 4060.
Ce script v√©rifie la d√©tection, les performances et l'optimisation GPU.
"""

import torch
import time
import numpy as np
from agent.dqn_agent import DQNAgent

def test_gpu_detection():
    """Teste la d√©tection et l'utilisation du GPU."""
    
    print("üîç TEST DE D√âTECTION GPU RTX 4060")
    print("="*50)
    
    # V√©rifications de base
    print(f"PyTorch version: {torch.__version__}")
    print(f"CUDA disponible: {torch.cuda.is_available()}")
    
    if not torch.cuda.is_available():
        print("‚ùå CUDA non disponible - V√©rifiez votre installation")
        return False, None
    
    # D√©tection des GPU
    gpu_count = torch.cuda.device_count()
    print(f"Nombre de GPU: {gpu_count}")
    
    rtx_4060_found = False
    rtx_4060_index = None
    
    for i in range(gpu_count):
        gpu_name = torch.cuda.get_device_name(i)
        gpu_memory = torch.cuda.get_device_properties(i).total_memory / (1024**3)
        
        print(f"\nGPU {i}: {gpu_name}")
        print(f"  M√©moire: {gpu_memory:.1f} GB")
        print(f"  Compute Capability: {torch.cuda.get_device_properties(i).major}.{torch.cuda.get_device_properties(i).minor}")
        
        if "4060" in gpu_name or "RTX 4060" in gpu_name:
            rtx_4060_found = True
            rtx_4060_index = i
            print(f"  ‚úÖ RTX 4060 TROUV√âE !")
    
    if rtx_4060_found:
        print(f"\nüéâ RTX 4060 d√©tect√©e sur GPU {rtx_4060_index}")
        return True, rtx_4060_index
    else:
        print(f"\n‚ö†Ô∏è RTX 4060 non d√©tect√©e par nom - Utilisation GPU 0")
        return True, 0

def test_agent_gpu_usage():
    """Teste l'utilisation GPU par l'agent DQN."""
    
    print(f"\nüß† TEST D'UTILISATION GPU PAR L'AGENT DQN")
    print("="*45)
    
    # Cr√©er un agent
    agent = DQNAgent(state_size=11, action_size=3)
    
    # V√©rifier le device utilis√©
    device_info = agent.get_model_info()
    print(f"Device de l'agent: {device_info['device']}")
    print(f"Param√®tres du mod√®le: {device_info['total_parameters']:,}")
    
    # Test avec un √©tat simul√©
    test_state = np.random.random(11)
    
    # Mesurer le temps d'inf√©rence
    start_time = time.time()
    for _ in range(1000):
        action = agent.get_action(test_state, training=False)
    inference_time = time.time() - start_time
    
    print(f"Temps pour 1000 inf√©rences: {inference_time:.3f}s")
    print(f"Vitesse: {1000/inference_time:.1f} inf√©rences/seconde")
    
    return True

def benchmark_gpu_vs_cpu():
    """Compare les performances GPU vs CPU."""
    
    print(f"\n‚ö° BENCHMARK GPU vs CPU")
    print("="*30)
    
    # Test matrices pour simulation de l'entra√Ænement
    matrix_size = 2048
    num_iterations = 10
    
    print(f"Test: Multiplication de matrices {matrix_size}x{matrix_size}")
    print(f"Iterations: {num_iterations}")
    
    # Test CPU
    print(f"\nüêå Test CPU...")
    cpu_times = []
    for i in range(num_iterations):
        start = time.time()
        a = torch.randn(matrix_size, matrix_size)
        b = torch.randn(matrix_size, matrix_size)
        c = torch.mm(a, b)
        cpu_times.append(time.time() - start)
    
    avg_cpu_time = np.mean(cpu_times)
    print(f"Temps moyen CPU: {avg_cpu_time:.3f}s")
    
    # Test GPU
    if torch.cuda.is_available():
        print(f"üöÄ Test GPU...")
        device = torch.device("cuda")
        gpu_times = []
        
        # Warm-up GPU
        a_gpu = torch.randn(matrix_size, matrix_size, device=device)
        b_gpu = torch.randn(matrix_size, matrix_size, device=device)
        _ = torch.mm(a_gpu, b_gpu)
        torch.cuda.synchronize()
        
        for i in range(num_iterations):
            start = time.time()
            a_gpu = torch.randn(matrix_size, matrix_size, device=device)
            b_gpu = torch.randn(matrix_size, matrix_size, device=device)
            c_gpu = torch.mm(a_gpu, b_gpu)
            torch.cuda.synchronize()
            gpu_times.append(time.time() - start)
        
        avg_gpu_time = np.mean(gpu_times)
        speedup = avg_cpu_time / avg_gpu_time
        
        print(f"Temps moyen GPU: {avg_gpu_time:.3f}s")
        print(f"üèÜ Acc√©l√©ration: {speedup:.1f}x plus rapide")
        
        if speedup > 5:
            print("‚úÖ Excellente performance GPU !")
        elif speedup > 2:
            print("‚úÖ Bonne performance GPU")
        else:
            print("‚ö†Ô∏è Performance GPU d√©cevante - V√©rifiez les drivers")
    
    return True

def test_memory_usage():
    """Teste l'utilisation de la m√©moire GPU."""
    
    print(f"\nüíæ TEST DE M√âMOIRE GPU")
    print("="*25)
    
    if torch.cuda.is_available():
        device = torch.device("cuda")
        
        # M√©moire avant
        torch.cuda.empty_cache()
        memory_before = torch.cuda.memory_allocated(device) / (1024**2)
        memory_total = torch.cuda.get_device_properties(device).total_memory / (1024**2)
        
        print(f"M√©moire utilis√©e avant: {memory_before:.1f} MB")
        print(f"M√©moire totale: {memory_total:.1f} MB")
        
        # Cr√©er un agent et tester
        agent = DQNAgent(state_size=11, action_size=3)
        
        # M√©moire apr√®s cr√©ation de l'agent
        memory_after = torch.cuda.memory_allocated(device) / (1024**2)
        memory_used = memory_after - memory_before
        
        print(f"M√©moire utilis√©e par l'agent: {memory_used:.1f} MB")
        print(f"Pourcentage utilis√©: {(memory_after/memory_total)*100:.2f}%")
        
        # Test de charge m√©moire avec batch
        print(f"\nTest avec batch d'entra√Ænement...")
        batch_size = 32
        state_size = 11
        
        states = torch.randn(batch_size, state_size, device=device)
        memory_batch = torch.cuda.memory_allocated(device) / (1024**2)
        
        print(f"M√©moire avec batch: {memory_batch:.1f} MB")
        
        # Nettoyage
        del agent, states
        torch.cuda.empty_cache()
        
        print("‚úÖ Test m√©moire termin√©")
    else:
        print("‚ùå GPU non disponible pour test m√©moire")

def main():
    """Fonction principale de test."""
    
    print("üéÆ TEST COMPLET D'OPTIMISATION RTX 4060")
    print("="*55)
    
    try:
        # Test 1: D√©tection GPU
        gpu_result = test_gpu_detection()
        
        if gpu_result == (False, None):
            print("\n‚ùå Tests interrompus - GPU non disponible")
            return
        
        gpu_available, gpu_index = gpu_result
        
        # Test 2: Agent DQN
        test_agent_gpu_usage()
        
        # Test 3: Benchmark
        benchmark_gpu_vs_cpu()
        
        # Test 4: M√©moire
        test_memory_usage()
        
        print(f"\nüéâ TESTS TERMIN√âS AVEC SUCC√àS !")
        print("="*35)
        print("‚úÖ Votre RTX 4060 est pr√™te pour l'entra√Ænement Snake RL")
        print("‚ö° Performance optimale garantie")
        print("üöÄ Lancez maintenant: python train.py ou python quick_train.py")
        
    except Exception as e:
        print(f"\n‚ùå Erreur pendant les tests: {e}")
        print("üí° V√©rifiez votre installation PyTorch et drivers NVIDIA")

if __name__ == "__main__":
    main()
