# Snake RL - Apprentissage par Renforcement

ğŸ **Une IA qui apprend Ã  jouer Ã  Snake via Deep Q-Learning (DQN)**  
âš¡ **OptimisÃ© pour GPU NVIDIA (RTX 4060 et plus)**

## ğŸ¯ Objectif

Ce projet implÃ©mente un agent d'apprentissage par renforcement qui apprend autonomement Ã  jouer au jeu Snake en utilisant l'algorithme Deep Q-Network (DQN) avec PyTorch. L'entraÃ®nement est **accÃ©lÃ©rÃ© par GPU** pour des performances optimales.

## ğŸš€ FonctionnalitÃ©s

- âœ… **Environnement Snake** personnalisÃ© compatible Gymnasium
- âœ… **Agent DQN** avec target network et experience replay
- âœ… **AccÃ©lÃ©ration GPU** - DÃ©tection automatique RTX 4060/RTX 30-40 Series
- âœ… **EntraÃ®nement ultra-rapide** - 15-20 min vs 2-3h (10x plus rapide)
- âœ… **Mode de jeu manuel** - Jouez vous-mÃªme pour comparer
- âœ… **Tests et comparaisons** de performances dÃ©taillÃ©s
- âœ… **Analyse** complÃ¨te avec visualisations
- âœ… **Sauvegarde/Chargement** de modÃ¨les optimisÃ©s
- âœ… **Notebook pÃ©dagogique** complet pour dÃ©butants

## ğŸ“¦ Technologies

- **Python 3.8+**
- **PyTorch** - Deep learning framework (CUDA optimisÃ©)
- **Pygame** - Environnement de jeu
- **NumPy** - Calculs numÃ©riques  
- **Matplotlib** - Visualisation
- **Gymnasium** - API d'environnement RL
- **CUDA** - AccÃ©lÃ©ration GPU NVIDIA

## ğŸ® GPU Support

### GPU SupportÃ©s
- âœ… **RTX 40 Series** (4060, 4070, 4080, 4090) - RecommandÃ©
- âœ… **RTX 30 Series** (3060, 3070, 3080, 3090)
- âœ… **RTX 20 Series** (2060, 2070, 2080)
- âœ… **GTX 16 Series** (1660, 1650) - Performance rÃ©duite
- âš ï¸ **CPU seulement** - Fonctionnel mais plus lent

### Performances GPU vs CPU
| Configuration | EntraÃ®nement 100 Ã©pisodes | EntraÃ®nement 2000 Ã©pisodes |
|--------------|---------------------------|----------------------------|
| **RTX 4060** | **30-60 sec** | **15-20 min** |
| **RTX 3070** | **25-50 sec** | **12-18 min** |
| **CPU seul** | 5-10 min | 2-3 heures |

## ğŸ—ï¸ Structure du Projet

```
snake-rl/
â”œâ”€â”€ env/                    # Environnement Snake
â”‚   â””â”€â”€ snake_game.py      # Classe SnakeGame (Gymnasium compatible)
â”œâ”€â”€ agent/                  # Agent d'apprentissage
â”‚   â””â”€â”€ dqn_agent.py       # Agent DQN avec GPU optimization
â”œâ”€â”€ models/                 # ModÃ¨les sauvegardÃ©s (crÃ©Ã© automatiquement)
â”œâ”€â”€ .vscode/                # Configuration VS Code
â”‚   â”œâ”€â”€ tasks.json         # TÃ¢ches d'entraÃ®nement/test
â”‚   â””â”€â”€ settings.json      # ParamÃ¨tres optimisÃ©s
â”œâ”€â”€ train.py               # Script d'entraÃ®nement (GPU optimisÃ©)
â”œâ”€â”€ quick_train.py         # EntraÃ®nement rapide (100 Ã©pisodes)
â”œâ”€â”€ test.py                # Script de test et Ã©valuation
â”œâ”€â”€ test_gpu.py            # Test des performances GPU
â”œâ”€â”€ play_manual.py         # Mode de jeu humain
â”œâ”€â”€ demo.py                # DÃ©monstration rapide
â”œâ”€â”€ utils.py               # Fonctions utilitaires et analyse
â”œâ”€â”€ Snake_RL_Guide_Complet.ipynb  # Notebook pÃ©dagogique
â””â”€â”€ requirements.txt       # DÃ©pendances Python avec CUDA
```

## âš¡ Installation Rapide

### PrÃ©requis
- **Python 3.8+**
- **GPU NVIDIA** avec drivers rÃ©cents (recommandÃ©)
- **8 GB RAM** minimum (16 GB recommandÃ©)

### Installation Automatique

1. **Cloner le projet**
```bash
git clone <votre-repo>
cd snake-rl
```

2. **Installer les dÃ©pendances (avec support GPU)**
```bash
pip install -r requirements.txt
```
*Note : Installe automatiquement PyTorch avec CUDA 11.8*

3. **VÃ©rifier la configuration GPU**
```bash
python test_gpu.py
```

4. **Test rapide (2 minutes)**
```bash
python quick_train.py
```

5. **EntraÃ®nement complet (15-20 min avec GPU)**
```bash
python train.py
```

### VS Code (RecommandÃ©)

Le projet inclut des tÃ¢ches VS Code prÃªtes Ã  l'emploi :

1. **Ouvrir dans VS Code**
2. **Ctrl+Shift+P** â†’ "Tasks: Run Task"
3. **Choisir :**
   - `Demo - Test Installation`
   - `Quick Train - 100 Episodes`  
   - `Full Training - 2000 Episodes`
   - `Test GPU RTX 4060`
   - `Test Agent`

## ğŸ® Utilisation

### EntraÃ®nement

```bash
# EntraÃ®nement standard (2000 Ã©pisodes)
python train.py

# Les modÃ¨les sont sauvegardÃ©s dans models/
# - snake_dqn.pth : modÃ¨le final
# - snake_dqn_best.pth : meilleur modÃ¨le
```

### Test et Ã‰valuation

```bash
python test.py
```

Menu interactif avec options :
1. **Test visuel** - Voir l'agent jouer
2. **Comparaison** - Agent entraÃ®nÃ© vs alÃ©atoire  
3. **Mode interactif** - ContrÃ´le manuel du test
4. **Analyse** - Graphiques d'entraÃ®nement dÃ©taillÃ©s

### Exemples d'Utilisation

```python
# Charger et utiliser un agent entraÃ®nÃ©
from env.snake_game import SnakeGame
from agent.dqn_agent import DQNAgent

env = SnakeGame()
agent = DQNAgent()
agent.load_model("models/snake_dqn_best.pth")

state, _ = env.reset()
action = agent.get_action(state, training=False)
```

## ğŸ§  Architecture de l'Agent

### RÃ©seau de Neurones (DQN)
- **EntrÃ©e** : Ã‰tat du jeu (11 features)
  - Dangers (straight, left, right)
  - Direction actuelle (4 directions)
  - Position relative de la nourriture (4 directions)
- **Couches cachÃ©es** : 256 â†’ 256 neurones (ReLU)
- **Sortie** : Q-values pour 3 actions (tout droit, droite, gauche)

### Algorithme DQN
- âœ… **Îµ-greedy** exploration policy
- âœ… **Target network** pour la stabilitÃ©
- âœ… **Experience replay** buffer (10k expÃ©riences)
- âœ… **Gradient clipping** pour Ã©viter l'instabilitÃ©

### HyperparamÃ¨tres par DÃ©faut
```python
learning_rate = 0.001
gamma = 0.9              # Facteur de discount
epsilon = 1.0 â†’ 0.01     # Exploration dÃ©croissante
batch_size = 32
memory_size = 10000
target_update = 1000     # FrÃ©quence de mise Ã  jour du target network
```

## ğŸ“Š RÃ©sultats Attendus

Un agent bien entraÃ®nÃ© devrait atteindre :
- **Score moyen** : 10-20+ (vs ~1 pour un agent alÃ©atoire)
- **Score maximum** : 30-50+
- **Convergence** : visible aprÃ¨s 500-1000 Ã©pisodes

## ğŸ›ï¸ Configuration AvancÃ©e

### Modification des HyperparamÃ¨tres

Ã‰ditez directement dans `train.py` :

```python
agent = DQNAgent(
    lr=0.0005,           # Learning rate plus faible
    gamma=0.95,          # Facteur de discount plus Ã©levÃ©
    epsilon_decay=0.99,  # DÃ©croissance plus lente
    batch_size=64        # Batch plus grand
)
```

### Reward Shaping

Modifiez la fonction `step()` dans `env/snake_game.py` :

```python
# Ajouter des rÃ©compenses intermÃ©diaires
if self.head == self.food:
    reward = 10
else:
    # RÃ©compense pour se rapprocher de la nourriture
    reward = self._get_food_reward()
```

### Architecture du RÃ©seau

Modifiez la classe `DQN` dans `agent/dqn_agent.py` :

```python
class DQN(nn.Module):
    def __init__(self, input_size=11, hidden_size=512, output_size=3):
        super().__init__()
        self.linear1 = nn.Linear(input_size, hidden_size)
        self.linear2 = nn.Linear(hidden_size, hidden_size)
        self.linear3 = nn.Linear(hidden_size, hidden_size)  # Couche supplÃ©mentaire
        self.linear4 = nn.Linear(hidden_size, output_size)
```

## ğŸ”§ Troubleshooting

### ProblÃ¨mes Courants

**L'agent n'apprend pas :**
- VÃ©rifiez le learning rate (essayez 0.0001-0.01)
- Augmentez la taille du replay buffer
- RÃ©duisez epsilon_decay pour plus d'exploration

**EntraÃ®nement instable :**
- Activez le gradient clipping
- RÃ©duisez le learning rate  
- Augmentez la frÃ©quence de mise Ã  jour du target network

**Performances faibles :**
- VÃ©rifiez la reprÃ©sentation de l'Ã©tat
- Ajustez les rÃ©compenses (reward shaping)
- Augmentez la taille du rÃ©seau

## ğŸš€ AmÃ©liorations Possibles

### Algorithmes RL AvancÃ©s
- [ ] **Double DQN** - RÃ©duction du biais d'optimisation
- [ ] **Dueling DQN** - SÃ©paration Value/Advantage
- [ ] **Prioritized Experience Replay** - Ã‰chantillonnage intelligent
- [ ] **Rainbow DQN** - Combinaison de toutes les amÃ©liorations

### Environnement
- [ ] **Obstacles** - Complexifier le jeu
- [ ] **Multi-snake** - Environnement multi-agents
- [ ] **Variantes** - DiffÃ©rentes tailles, vitesses
- [ ] **Curriculum Learning** - DifficultÃ© progressive

### Interface
- [ ] **Dashboard Streamlit** - Interface web
- [ ] **Enregistrement vidÃ©o** - Sauvegarder les parties
- [ ] **API REST** - Serveur pour dÃ©mo
- [ ] **Comparaison en ligne** - Leaderboard

## ğŸ“ˆ MÃ©triques et Analyse

Le projet inclut des outils d'analyse avancÃ©s :

```python
from utils import calculate_learning_metrics, plot_comparison

# Analyser les rÃ©sultats
scores = np.load("models/training_scores.npy")
metrics = calculate_learning_metrics(scores)

# Comparer plusieurs expÃ©riences
experiments = {
    'DQN_base': scores1,
    'DQN_tuned': scores2
}
plot_comparison(experiments)
```

## ğŸ¤ Contribution

Les contributions sont bienvenues ! 

1. Fork le projet
2. CrÃ©ez une branche feature (`git checkout -b feature/AmazingFeature`)
3. Commit (`git commit -m 'Add some AmazingFeature'`)
4. Push (`git push origin feature/AmazingFeature`)
5. Ouvrez une Pull Request

## ğŸ“„ Licence

Ce projet est sous licence MIT. Voir `LICENSE` pour plus de dÃ©tails.

## ğŸ”— RÃ©fÃ©rences

- [Deep Q-Learning Paper](https://arxiv.org/abs/1312.5602)
- [Gymnasium Documentation](https://gymnasium.farama.org/)
- [PyTorch RL Tutorial](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html)

---

**CrÃ©Ã© avec â¤ï¸ pour l'apprentissage par renforcement**
